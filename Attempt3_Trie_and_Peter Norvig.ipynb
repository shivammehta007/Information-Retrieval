{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checker\n",
    "#### Edit 1: Stupid Naive approach (Code deleted, it was just looping over everything maybe will take one year to solve this problem)\n",
    "#### Edit 2: Peter Norvich's spell checker\n",
    "#### Edit 3: Added Trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/itmo-spelling-correction-autumn-2019/evaluate.py\n",
      "/kaggle/input/itmo-spelling-correction-autumn-2019/no_fix.submission.csv\n",
      "/kaggle/input/itmo-spelling-correction-autumn-2019/words.csv\n",
      "/kaggle/input/itmo-spelling-correction-autumn-2019/train.csv\n",
      "Collecting textdistance\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/18/31397b687f50ffae65469175f07faa68f288e27fcd8716276004c42e5637/textdistance-4.1.5-py3-none-any.whl\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.1.5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "# Required Distance\n",
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "import time\n",
    "import csv\n",
    "import tqdm\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_file = '/kaggle/input/itmo-spelling-correction-autumn-2019/words.csv'\n",
    "train_file = '/kaggle/input/itmo-spelling-correction-autumn-2019/no_fix.submission.csv'\n",
    "truth_file = '/kaggle/input/itmo-spelling-correction-autumn-2019/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "724512it [00:05, 126024.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Word Dict\n",
    "words = {}\n",
    "\n",
    "with open(word_file, newline='') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm.tqdm(buffer):\n",
    "        words[r['Id']] = int(r['Freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362256it [00:02, 139290.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Truth File\n",
    "truth_values = set()\n",
    "\n",
    "with open(truth_file, newline='') as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm.tqdm(buffer):\n",
    "        truth_values.add(r['Expected'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332090"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(truth_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_to_keep = [1, 2]\n",
    "distance_function = textdistance.levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = words\n",
    "\n",
    "def correction(initial_word, threshold=1000):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    probability_dict = {}\n",
    "    # Probability for just the word\n",
    "    for word in known([initial_word]):\n",
    "        if word in WORDS:\n",
    "            if WORDS[word] > threshold:\n",
    "                probability_dict[word] = WORDS[word]\n",
    "        \n",
    "    \n",
    "    # Probability for one edit\n",
    "    for word in known(edits1(initial_word)):\n",
    "        if word in WORDS:\n",
    "            if WORDS[word] > threshold:\n",
    "                probability_dict[word] = WORDS[word]\n",
    "    \n",
    "#     # Probability for two edits\n",
    "#     for word in known(edits2(word)):\n",
    "#         probability_dict[word] = WORDS[word]\n",
    "#     print(f'Prob dictionary: {probability_dict}')\n",
    "\n",
    "    if probability_dict:\n",
    "        correction = sorted(probability_dict.items(), key=operator.itemgetter(1), reverse=True)[0][0]\n",
    "    else:\n",
    "        correction = initial_word\n",
    "\n",
    "    return correction\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    if words:\n",
    "        return set(w for w in words if w and w in truth_values)\n",
    "    return set()\n",
    "\n",
    "        \n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'бвгджзклмнпрстфхцчшщаэыуояеёюийьъabcdefghijklmnopqrstuvwxyz'.upper()\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'АНИМЕ'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('АНІМЕ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNIZOOI'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('UNIZOOI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing This"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works lets try on all test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "362256it [00:02, 145386.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "with open(train_file) as file:\n",
    "    buffer = csv.DictReader(file)\n",
    "    for r in tqdm.tqdm(buffer):\n",
    "        train.append(r['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362256/362256 [03:56<00:00, 1529.04it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('submission_pn.csv', 'w', newline='') as file:\n",
    "    buffer = csv.writer(file)\n",
    "    buffer.writerow(['Id', 'Predicted'])\n",
    "    for word in tqdm.tqdm(train):\n",
    "        corr = correction(word, 4000)\n",
    "        buffer.writerow([word, corr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "My just Trie Implementation: https://github.com/shivammehta007/pythondsa/blob/master/structures/trie.py\n",
    "This is just addon to the normal Trie\n",
    "\"\"\"\n",
    "\n",
    "class Trie:\n",
    "    \"\"\"\n",
    "    Trie\n",
    "\n",
    "    Trie is a treelike datastructure used to predict suggestion here is a\n",
    "    simple implementation of it with add and search functionality.\n",
    "\n",
    "    It has a root that is the head or the Trie and a node_count to count\n",
    "    total number of nodes currently in Trie\n",
    "    \"\"\"\n",
    "\n",
    "    class _Node:\n",
    "        \"\"\"\n",
    "        Node\n",
    "\n",
    "        This is how a trie node looks like it has a hashmap of characters\n",
    "        with an end indicating weather this is the end of word or not.\n",
    "        One additional field that I added is the frequency count just for\n",
    "        furthur probabilistic calculations if required.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, end=False):\n",
    "            self.characters = {}\n",
    "            self.frequency = 0\n",
    "            self.end = end\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root = self._Node()\n",
    "        self.node_count = 1\n",
    "\n",
    "\n",
    "    def add_string(self, string):\n",
    "        \"\"\"\n",
    "        Adds a string to the trie\n",
    "        Parameters:\n",
    "        string: String\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        for c in string:\n",
    "            if c not in node.characters:\n",
    "                node.characters[c] = self._Node()\n",
    "                self.node_count += 1\n",
    "\n",
    "            node = node.characters[c]\n",
    "\n",
    "        node.end = True\n",
    "        if string in WORDS:\n",
    "            node.frequency = WORDS[string]\n",
    "\n",
    "    def search_word(self, string):\n",
    "        \"\"\"\n",
    "        Searches for a word in the trie\n",
    "        Parameters:\n",
    "        string: String\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        for c in string:\n",
    "            if c not in node.characters:\n",
    "                return False, 0\n",
    "            node = node.characters[c]\n",
    "\n",
    "        if node.end:\n",
    "            return True, node.frequency\n",
    "\n",
    "        return False, 0\n",
    "\n",
    "    def one_edit_distance(self, word):\n",
    "        \"\"\"\n",
    "        All edits that are one edit away from word\n",
    "        Parameters:\n",
    "        word: String\n",
    "        \"\"\"\n",
    "        \n",
    "        letters    = 'бвгджзклмнпрстфхцчшщаэыуояеёюийьъabcdefghijklmnopqrstuvwxyz'.upper()\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    \n",
    "    def correct_spelling(self, initial_word, threshold=1000):\n",
    "        \"\"\"\n",
    "        Checks and Returns the Correct Spelling of the Word with one edit distance\n",
    "        \"\"\"\n",
    "        probability_dict = {}\n",
    "        present, _ = self.search_word(initial_word)\n",
    "        if present:\n",
    "            return initial_word\n",
    "    \n",
    "        for word in self.one_edit_distance(initial_word):\n",
    "            present, freq = self.search_word(word)\n",
    "            if not present:\n",
    "                continue\n",
    "            probability_dict[word] = freq\n",
    "            \n",
    "\n",
    "        if probability_dict:\n",
    "            correction = sorted(probability_dict.items(), key=operator.itemgetter(1), reverse=True)[0][0]\n",
    "        else:\n",
    "            correction = initial_word\n",
    "        \n",
    "        return correction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = Trie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332090/332090 [00:13<00:00, 24794.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm.tqdm(truth_values):\n",
    "    trie.add_string(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 314178)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.search_word('АНИМЕ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'КИТАЙ'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie.correct_spelling('КИТАЦ') # КИТАЙ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Create the final Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362256/362256 [15:42<00:00, 384.37it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('submission_trie.csv', 'w', newline='') as file:\n",
    "    buffer = csv.writer(file)\n",
    "    buffer.writerow(['Id', 'Predicted'])\n",
    "    for word in tqdm.tqdm(train):\n",
    "        corr = trie.correct_spelling(word)\n",
    "        buffer.writerow([word, corr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
